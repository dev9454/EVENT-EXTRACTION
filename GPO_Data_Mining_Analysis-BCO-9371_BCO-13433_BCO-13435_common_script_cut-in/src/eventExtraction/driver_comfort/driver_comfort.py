# -*- coding: utf-8 -*-
"""
Created on Fri Apr  4 13:36:28 2025

@author: mfixlz
"""
import operator

import logging as log
import random
# import traceback as tb
import numpy as np
# import datetime
import os
import sys
from typing import List
import itertools
from functools import reduce  # forward compatibility for Python 3
from operator import getitem
from collections.abc import Iterable
from collections import ChainMap
import scipy as sp


import time
import yaml
from datetime import timedelta, datetime
import re
import math
import copy
import json
from pathlib import Path
import pickle

# Give path were modules present
if "lin" in sys.platform:
    print('platform :', sys.platform)
    to_change_path = os.path.dirname(os.path.abspath(__file__))
    os.chdir(to_change_path)
    # sys.path.insert(
    #     0, r'/mnt/usmidet/projects/STLA-THUNDER/7-Tools/DMA_Venv/CES_related/GPO_event_extraction/GPO_Data_Mining_Analysis/src')
    # from eventExtraction.cut_in.core_cut_in import coreEventExtractionCUTIN

    # sys.path.insert(0, r'/mnt/usmidet/users/mfixlz/scripts/')
    sys.path.append(r'/mnt/usmidet/projects/FCA-CADM/7-Tools')
    sys.path.insert(1, os.path.join(sys.path[0], '../../'))
    # sys.stdout = open(os.devnull, 'w')
    # sys.stdout = sys.__stdout__
    from openpyxl import load_workbook, Workbook
    import pandas as pd
    from scipy.io import loadmat as load_mat_scipy
    # from joblib import Parallel, delayed, parallel_backend, parallel_config
    # from tqdm import tqdm
    import contextlib
    # import joblib
    # from ray.util.joblib import register_ray


elif "win" in sys.platform:
    to_change_path = os.path.dirname(os.path.abspath(__file__))
    os.chdir(to_change_path)

    # from utils_local.support_functions import find_index, array_at_mat_dict

    sys.path.insert(
        0, r'C:\Users\mfixlz\OneDrive - Aptiv\Documents\DM_A\PO_Chaitanya_K\Projects\GPO Data Mining Analysis\GPO_Data_Mining_Analysis\src')
    # from eventExtraction.cut_in.core_cut_in import coreEventExtractionCUTIN
    print('platform :', sys.platform)
    # sys.stdout = sys.__stdout__
    from openpyxl import load_workbook, Workbook
    import pandas as pd
    from scipy.io import loadmat as load_mat_scipy
    from joblib import Parallel, delayed, parallel_backend, parallel_config
    from tqdm import tqdm
    import contextlib
    import joblib
    from ray.util.joblib import register_ray


class DriverComfort:
    """
    Class name and filename needs to be in sync with each other, 
    so that we can dynamically initialize this class.
    Example : file_name = sample_usr_script then class_name needs to be 
    SampleUsrScript
    User should add a general description of what this script does in here.
    **DESCRIPTION**

    functions that user can modify : __init__, run, kpi_sheet_generation, generate_kpi_df.
    functions that user cannot modify : get_func_name, get_headers, 
                                        get_min_value, get_std, get_mean.

    User can add more functions if needed.
    """

    def __init__(self):

        self._func_name = os.path.basename(__file__).replace(".py", "")

        self._venv = \
            '/mnt/usmidet/projects/FCA-CADM/7-Tools/DMA_Venv_test/bin/activate'

        # users to update list of headers as per their requirement.
        # This script supports single sub-sheet or multiple sub-sheet creation as per user's needs.
        # Update the list of headers with the column names you wish to see in the
        # output excel sheet generated by this event finder

        # Begin with "log_path" AND "log_name" in each sheet.

        # Below section can be modified.

        self._headers = dict()

        self._version = "1.0"

        common_headers = ['log_path',
                          'log_name',
                          ]
        # lateral_velocity_bins = np.arange(0, 3.2, 0.2)
        # longitudinal_velocity_bins = np.arange(0, 110, 10)
        # vertical_velocity_bins = np.arange(0, 0.505, 0.05)

        ['lateral_velocity_' + str()]
        # self._headers['output'] = ['log_path',
        #                            'log_name',
        #                            'lateral_acceleration_mean',
        #                            'lateral_acceleration_median',
        #                            'lateral_acceleration_std',

        #                            'lateral_jerk_mean',
        #                            'lateral_jerk_median',
        #                            'lateral_jerk_std',

        #                            'lateral_velocity_mean',
        #                            'lateral_velocity_median',
        #                            'lateral_velocity_std',

        #                            'longitudinal_acceleration_mean',
        #                            'longitudinal_acceleration_median',
        #                            'longitudinal_acceleration_std',

        #                            'longitudinal_jerk_mean',
        #                            'lateral_jerk_median',
        #                            'longitudinal_jerk_std',

        #                            'longitudinal_velocity_mean',
        #                            'longitudinal_velocity_median',
        #                            'longitudinal_velocity_std',

        #                            'vertical_acceleration_mean',
        #                            'vertical_acceleration_median',
        #                            'vertical_acceleration_std',

        #                            'vertical_jerk_mean',
        #                            'vertical_jerk_median',
        #                            'vertical_jerk_std',

        #                            'vertical_velocity_mean',
        #                            'vertical_velocity_median',
        #                            'vertical_velocity_std',

        #                            'yaw_rate_mean',
        #                            'yaw_rate_median',
        #                            'yaw_rate_std',

        #                            ]

        self._headers['output'] = ['log_path',
                                   'log_name',
                                   'full_pickle_path',
                                   # 'category',
                                   ]
        self.count_of_logs_to_look_back = 3

        self.lambda_value: float = 1E4
        self.polynomial_order: int = 2
        self.optimum_smooth: bool = True

        self.acceleration_due_to_gravity = 9.81  # [mps2]

        self.window_length_sec_trajectory_smoothness = 10  # [s]

        self.pre_event_window_length_sec = 10  # [s]

        ##################################################################
        if "win" in sys.platform:

            event_extraction_output_xlsx_path = \
                os.path.join(r"C:\Users\mfixlz\OneDrive - Aptiv\Documents",
                             r'DM_A\PO_Chaitanya_K\Projects',
                             r'GPO Data Mining Analysis\GPO_Data_Mining_Analysis',
                             r'src\eventExtraction\driver_comfort',
                             'ACC_events.xlsx')
        elif "lin" in sys.platform:
            self.model_output_path_only = \
                os.path.join(r"/mnt/usmidet/projects/STLA-THUNDER/7-Tools",
                             r"DMA_Venv/CES_related/GPO_event_extraction",
                             r"GPO_Data_Mining_Analysis",
                             r"src/eventExtraction/driver_comfort",)
            event_extraction_output_xlsx_path = \
                os.path.join(self.model_output_path_only,
                             'ACC_events.xlsx')

        self.event_start_cTime_col = 'event_start_cTime'
        self.description_col = 'Category'
        log_path_col = 'log_path'
        self.log_name_col = 'log_name'

        df_events = pd.read_excel(event_extraction_output_xlsx_path,
                                  sheet_name='output_data')

        self.df_events_req = df_events[[log_path_col,
                                        self.log_name_col,
                                        self.event_start_cTime_col,
                                        self.description_col
                                        ]]

        # description_to_weights_dict_ = {
        #     'host_braking' :
        #     }
        #################################################################

    def get_func_name(self):
        """
        :return: Returns function name
        """
        return self._func_name

    def get_venv(self):
        """
        Return virtual env from constructor
        """
        return self._venv

    def get_headers(self):
        """
        :return: returns headers
        """
        return self._headers

    def get_version(self):
        """
        :return: Return script version
        """
        return self._version

    def _timeseries_random_forest(self,
                                  # (n_samples, n_features, n_timestamps)
                                  X_train: np.array,
                                  # (n_samples,)
                                  y_train,
                                  kwargs_random_forest: dict = {}):
        from pyts.classification import TimeSeriesForest
        from pyts.multivariate.classification import MultivariateClassifier

        classifier_obj = MultivariateClassifier(
            TimeSeriesForest(**kwargs_random_forest))

        # Fitting the data (Training)

        classifier_obj.fit(X_train, y_train)

        return classifier_obj

    def kpi_sheet_generation(self, output_excel_sheet):
        """
        This function will be called from write_excel_to_excel and 
        take out excel as input and add KPI sheet to it.
        :param output_excel_sheet: excel sheet generated after reducer.
        :return:
        """
        from itertools import chain
        import random

        df = pd.read_excel(output_excel_sheet,
                           sheet_name='output')

        pickle_data = []

        req_time_series_len = len(df)
        test_train_split_ratio = 0.3
        test_data_indices = np.array(random.sample(range(req_time_series_len),
                                                   int(test_train_split_ratio
                                                       * req_time_series_len)))
        train_data_indices = np.array(list(set(range(req_time_series_len))
                                           - set(test_data_indices)))

        # get a different lists of each category and then train test split

        for item in df['full_pickle_path']:

            with open(item, "rb") as f:
                log_data = pickle.load(f)

            pickle_data.append(log_data)

        pickle_data = list(chain.from_iterable(pickle_data))

        # y_data = [df_ts['category'].to_numpy()
        #           for df_ts in pickle_data]

        # pickle_data = pd.concat(pickle_data, axis = 0, ignore_index=True)

        y_data = [item['category'].to_numpy()
                  for item in pickle_data]

        # y_data = np.concatenate(y_data, )

        X_data = [item[['jerk_component',
                        'g_force_component',
                        'trajectory_smoothness_component']].to_numpy()
                  for item in pickle_data]

        X_data = [np.reshape(item,
                             (1, item.shape[1], item.shape[0]))
                  for item in X_data]

        X_data_train = np.concatenate([X_data[idx]
                                       for idx in
                                       train_data_indices], axis=0)
        # y_data_train = np.concatenate([y_data[idx]
        #                                for idx in
        #                                train_data_indices])
        y_data_train = [np.unique(y_data[idx])[0]
                        for idx in
                        train_data_indices]

        X_data_test = np.concatenate([X_data[idx]
                                      for idx in
                                      test_data_indices], axis=0)
        # y_data_test = np.concatenate([y_data[idx]
        #                                for idx in
        #                                test_data_indices])
        y_data_test = [np.unique(y_data[idx])[0]
                       for idx in
                       test_data_indices]

        # X_data = np.concatenate(X_data, axis=0)

        fit_obj = self._timeseries_random_forest(X_data_train,
                                                 y_data_train)

        mean_accuracy = fit_obj.score(X_data_test, y_data_test)

        print('&&&&&&&&&&&&&&&&&&&&&&&&& \n',
              f'MEAN ACCURACY {mean_accuracy}')

        basename_to_save_pickle = 'random_forest_timeseries_classification'

        model_output_path = os.path.join(
            self.model_output_path_only,
            basename_to_save_pickle + '.pkl')

        model_output_path = self._check_duplicate_files(model_output_path,
                                                        basename_to_save_pickle,
                                                        file_extension='.pkl'
                                                        )

        # return fit_obj

    def get_dict_value_by_path(self, root_dict, key_sequence):
        """Access a nested object in root by item sequence."""
        try:
            out_val = reduce(getitem, key_sequence, root_dict)
            out_message = 'success'
        except (KeyError, TypeError, IndexError, MemoryError) as e:
            out_val = None
            # key_path = [key + '->' for key in key_sequence]
            out_message = f'Exception / error occured. The exception is {e}'
            # out_message = out_message + f'\n check if data exists in {key_path}'
            # print(out_message)
            # logging.error(
            #     'Error occured in get_dict_value_by_path(...)', exc_info=e)
        return out_val, out_message

    def _get_signal_paths(self, config_path):

        with open(os.path.join(config_path, )) as f:
            # use safe_load instead load
            signal_dict = yaml.safe_load(f)

        return signal_dict

    def _signal_mapping(self, config_path, file_with_path):

        signal_dict = self._get_signal_paths(config_path)

        if os.path.isfile(file_with_path):

            raw_data = self.loadmat(file_with_path)

        else:
            return None

        if 'cTime_multiplier' in signal_dict:

            cTime_multiplier = float(signal_dict.pop("cTime_multiplier"))
        else:
            cTime_multiplier = 1.0

        if 'events_dict' in signal_dict:

            events_dict = signal_dict.pop('events_dict')

        df_list = [self.transform_df(
            self._get_signals(val, raw_data))
            for val in signal_dict.values()]
        df_list.sort(key=len, reverse=True)

        df_list_2 = []
        for df_iter in df_list:

            if df_iter['cTime'].isnull().any():

                df_iter['cTime'] = df_iter['cTime'].fillna(method='ffill')

            df_list_2.append(df_iter)

        df = self.merge_pandas_df(df_list_2, merge_key_list='cTime')

        df['cTime'] = df['cTime']*cTime_multiplier

        return df

    def _get_signals(self, signal_dict_vals, raw_data):

        signal_dict_vals = {key: val
                            for key, val in signal_dict_vals.items()
                            # if bool(val.strip())
                            }

        signal_dict_extracted = {key: self.stream_check(raw_data, val)
                                 for key, val in signal_dict_vals.items()
                                 }

        return signal_dict_extracted

    def _apply_stats_functions(self,
                               numerical_array: np.array,
                               array_name: str):

        print(array_name)

        stats_dict = {'mean': np.nanmean,
                      'std': np.nanstd,
                      'median': np.median,
                      }

        print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')

        return_val = {array_name + '_' + key:
                      val(numerical_array)
                      if np.issubdtype(np.array(numerical_array).dtype,
                                       np.number)
                      else numerical_array[0]
                      for key, val in stats_dict.items()

                      }

        return return_val

    def _get_past_logs_data(self, file_name_only, file_path):

        # count_of_logs_to_look_back = 3

        file_name_splitted = file_name_only.split('_')

        file_series_number = file_name_splitted[-1].split('.')[0]
        # leading_zeroes_count = (len(file_series_number) -
        # len(file_series_number.lstrip('0')))

        # format_with_lead_0 = "{:0"+f"{leading_zeroes_count}d" + "}"

        total_length_of_series = len(file_series_number)

        file_series_number = int(file_series_number)
        file_series_extension = file_name_splitted[-1].split('.')[-1]

        series_list = [item for item in range(file_series_number,
                                              file_series_number -
                                              self.count_of_logs_to_look_back,
                                              # count_of_logs_to_look_back,
                                              -1)]
        series_list = [str(item).zfill(total_length_of_series)
                       for item in series_list if item >= 0]

        series_file_names = ['_'.join(file_name_splitted[:-1])
                             + '_' + item
                             + '.' + file_series_extension
                             for item in series_list
                             ]

        series_file_names.sort()

        total_file_paths = [os.path.join(file_path, file_name,)
                            for file_name in series_file_names]

        df_list = [self._signal_mapping(kwargs['config_path'],
                                        file_with_path)
                   for file_with_path in total_file_paths
                   ]

        max_index_missing_log_sequence = max((loc
                                              for loc, val in enumerate(df_list)
                                              if val is None),
                                             default=-1)

        df_list = df_list[max_index_missing_log_sequence + 1:]

        df = pd.concat(df_list, axis=0, ignore_index=True)

        return df

    def _get_smoothened_data_whittaker_eilers(self, df,
                                              lambda_value: float = 1E4,
                                              polynomial_order: int = 2,
                                              optimum_smooth: bool = True):

        whittaker_smoother = self.WhittakerSmoother(
            lmbda=lambda_value,
            order=polynomial_order,
            data_length=len(df),
            x_input=df['cTime']
        )

        for column in df.columns:

            if self.is_numeric_dtype(df[column]):
                if optimum_smooth:
                    df[column] = whittaker_smoother\
                        .smooth_optimal(df[column])\
                        .get_optimal().get_smoothed()
                else:

                    df[column] = whittaker_smoother\
                        .smooth(df[column])

        return df

    def _trajectory_smoothness_component_linear(self,
                                                df: pd.DataFrame,
                                                window_length: int):

        indexer = pd.api.indexers.FixedForwardWindowIndexer(
            window_size=window_length)

        trajectory_smoothness_component = ((1/df['cTime'].diff(periods=1)) *
                                           ((df['road_curvature']**2 +
                                             (df['road_curvature'].diff(
                                                 periods=1).bfill()
                                            /
                                            df['cTime'].diff(
                                                 periods=1).bfill()))
                                            .rolling(window=indexer,
                                                     min_periods=1).sum()).bfill()
                                           )

        return trajectory_smoothness_component

    def _g_force_component_linear(self,
                                  df: pd.DataFrame,
                                  w1: float,
                                  w2: float):

        if w1+w2 != 1:

            w2 = 1-w1

        g_force_component = w1*(np.maximum(
                                df['host_lateral_acceleration_mps2'],
                                df['host_longitudinal_acceleration_mps2']) /
                                self.acceleration_due_to_gravity
                                ) + w2*df.apply(
            lambda x:
            (x['host_longitudinal_velocity_mps']**2
             * x['road_curvature'])
            / self.acceleration_due_to_gravity,
            axis=1)

        return g_force_component

    def _jerk_component_linear(self,
                               df: pd.DataFrame,
                               w1: float,
                               w2: float):

        if w1+w2 != 1:

            w2 = 1-w1

        jerk_component = w1*df['host_lateral_jerk_mps3'] \
            + w2*df['host_longitudinal_jerk_mps3']

        return jerk_component

    def _check_duplicate_files(self,
                               path_to_save_file,
                               basename_to_save_file,
                               file_extension: str = '.pkl'
                               ):

        all_files = [file[:-len(file_extension)]
                     for file in os.listdir(path_to_save_file)
                     if file.endswith(file_extension)
                     and basename_to_save_file in file
                     ]

        if len(all_files) > 0:

            all_files.sort(reverse=False)

            print('********************************')
            print('EXISTING PICKLE FILES WITH SAME NAME \n')

            print(all_files)

            print('********************************')

            pkl_enum = int(all_files[-1].split('.')[-1][
                -(len(file_extension)-1):]) + 1

        else:

            pkl_enum = 0

        path_to_save_file = os.path.join(path_to_save_file,
                                         basename_to_save_file +
                                         f'_{pkl_enum:03}' + file_extension)

        return path_to_save_file

    def run(self, file_name, **kwargs):
        """
        This function is used for extracting events from input file.
        :param file_name: input file name
        :param kwargs: additional arguments required by below section
        :return:
        Author: Revanth B
        """
        try:
            if "lin" in sys.platform:
                sys.path.insert(
                    0, r'/mnt/usmidet/projects/STLA-THUNDER/7-Tools/DMA_Venv/CES_related/GPO_event_extraction/GPO_Data_Mining_Analysis/src')
                from eventExtraction.utils.utils_generic import (_time_duration_to_indices_length,
                                                                 loadmat,
                                                                 merge_pandas_df,
                                                                 stream_check,
                                                                 transform_df,
                                                                 find_closest_index)
            elif "win" in sys.platform:
                print('****************', os.getcwd())
                sys.path.insert(
                    0, r'C:\Users\mfixlz\OneDrive - Aptiv\Documents\DM_A\PO_Chaitanya_K\Projects\GPO Data Mining Analysis\GPO_Data_Mining_Analysis\src')
                from eventExtraction.utils.utils_generic import (_time_duration_to_indices_length,
                                                                 loadmat,
                                                                 merge_pandas_df,
                                                                 stream_check,
                                                                 transform_df,
                                                                 find_closest_index)
            from eventExtraction.utils.whittaker_eilers import (whittaker_eilers,
                                                                WhittakerSmoother,
                                                                # CrossValidationResult,
                                                                # OptimisedSmoothResult
                                                                )
            from pandas.api.types import (is_object_dtype,
                                          is_numeric_dtype, is_bool_dtype)

            file_path, file_name_only = os.path.split(file_name)

            self.stream_check = stream_check
            self.transform_df = transform_df
            self.loadmat = loadmat
            self.merge_pandas_df = merge_pandas_df
            self.WhittakerSmoother = WhittakerSmoother
            self.is_numeric_dtype = is_numeric_dtype

            # self.TimeSeriesForest = TimeSeriesForest

            # self.raw_data = loadmat(file_name)

            df_total = self._get_past_logs_data(file_name_only, file_path)

            if (pd.api.types.infer_dtype(
                    df_total["host_longitudinal_jerk_mps3"]) == 'string'
                or
                    len(df_total["host_longitudinal_jerk_mps3"].unique()) == 1):

                df_total['host_longitudinal_jerk_mps3'] = (
                    df_total['host_longitudinal_acceleration_mps2'].diff(
                        periods=1).bfill()
                    /
                    df_total['cTime'].diff(
                        periods=1).bfill())

            df_total = self._get_smoothened_data_whittaker_eilers(df_total,
                                                                  self.lambda_value,
                                                                  self.polynomial_order,
                                                                  self.optimum_smooth)

            df_total['jerk_component'] = self._jerk_component_linear(df_total,
                                                                     w1=0.5,
                                                                     w2=0.5)

            df_total['g_force_component'] = self._g_force_component_linear(
                df_total,
                w1=0.5,
                w2=0.5)

            window_indices_length_trajectory_smoothness = \
                _time_duration_to_indices_length(df_total,
                                                 self.window_length_sec_trajectory_smoothness)

            df_total['trajectory_smoothness_component'] = \
                self._trajectory_smoothness_component_linear(
                df_total,
                window_length=window_indices_length_trajectory_smoothness)

            req_row = self.df_events_req[
                self.df_events_req[self.log_name_col]
                == file_name_only]

            req_cTime_list = req_row[self.event_start_cTime_col].to_list()

            req_indices_list = [
                find_closest_index(cTime_val, df_total, 'cTime')[0]
                for cTime_val in req_cTime_list]

            # self.

            pre_event_window_indices_length = \
                _time_duration_to_indices_length(df_total,
                                                 self.pre_event_window_length_sec)

            req_look_back_indices = [max(idx - pre_event_window_indices_length,
                                         df_total.index.min())
                                     for idx in req_indices_list]
            time_series_df_list = [df_total.copy(deep=True)[
                ['jerk_component',
                 'g_force_component',
                 'trajectory_smoothness_component', ]].loc[start:end, :]
                for start, end in zip(req_look_back_indices,
                                      req_indices_list)
            ]

            time_series_df_list_ = []
            for df_time_series, time in zip(time_series_df_list,
                                            req_cTime_list):

                df_time_series['category'] = \
                    self.df_events_req[
                        self.df_events_req[
                            self.event_start_cTime_col]
                        == time][self.description_col].values[0]
                # time_series_df_list_.append(df_time_series)

            df_total = None

            basename_to_save_pickle = file_name_only.split('.')[0] \
                + '_driver_comfort'

            path_to_save_pickle = kwargs['path_to_save_pickle']

            pickle_out_path = self._check_duplicate_files(path_to_save_pickle,
                                                          basename_to_save_pickle,
                                                          file_extension='.pkl'
                                                          )

            # all_pickle_files = [file[:-4]
            #                     for file in os.listdir(path_to_save_pickle)
            #                     if file.endswith(".pkl")
            #                     and basename_to_save_pickle in file
            #                     ]

            # if len(all_pickle_files) > 0:

            #     all_pickle_files.sort(reverse=False)

            #     print('********************************')
            #     print('EXISTING PICKLE FILES WITH SAME NAME \n')

            #     print(all_pickle_files)

            #     print('********************************')

            #     pkl_enum = int(all_pickle_files[-1].split('.')[-1][-3:]) + 1

            # else:

            #     pkl_enum = 0

            # pickle_out_path = os.path.join(path_to_save_pickle,
            #                                basename_to_save_pickle +
            #                                f'_{pkl_enum:03}' + '.pkl')

            # pickle_data = time_series_df_list

            with open(pickle_out_path, 'wb') as file:

                # A new file will be created
                pickle.dump(time_series_df_list, file)

            data_out = np.array([file_path,
                                 file_name_only,
                                 pickle_out_path,
                                 ])

            complete_events_dict = {key: value
                                    if isinstance(value, np.ndarray)
                                    or isinstance(value, list)
                                    else [value]
                                    for key, value in
                                    zip(self._headers['output'],
                                        data_out)}

            complete_events_df = pd.DataFrame(complete_events_dict)

            if 'local' in kwargs and kwargs['local']:
                return_val = complete_events_df

            else:
                return_val = data_out
            # x_input = df_total['cTime']
            # lat_accel = df_total['host_lateral_acceleration_mps2']
            # whittaker_smoother = WhittakerSmoother(
            #     lmbda=2e4, order=2, data_length=len(lat_accel), x_input=x_input
            # )
            # optimal_smooth = whittaker_smoother.smooth_optimal(lat_accel)
            # smoothened_data = optimal_smooth.get_optimal().get_smoothed()

            # req_signals_df = self._signal_mapping(
            #     kwargs['config_path'], file_name)

            # # req_signals_df = req_signals_df.apply(pd.to_numeric,
            # #                            errors='ignore',
            # #                            downcast='unsigned')

            # lateral_columns = [col
            #                    for col in req_signals_df.columns
            #                    if 'lateral' in col]
            # lateral_columns.sort()

            # longitudinal_columns = [col
            #                         for col in req_signals_df.columns
            #                         if 'longitudinal' in col]
            # longitudinal_columns.sort()

            # vertical_columns = [col
            #                     for col in req_signals_df.columns
            #                     if 'vertical' in col]
            # vertical_columns.sort()

            # yaw_columns = [col
            #                for col in req_signals_df.columns
            #                if 'yaw' in col]
            # yaw_columns.sort()

            # stats = [self._apply_stats_functions(
            #     np.array(req_signals_df[item]), item)
            #     for req_columns in
            #     [lateral_columns,
            #      longitudinal_columns,
            #      vertical_columns,
            #      yaw_columns,
            #      ]
            #     for item in req_columns

            # ]

            # stats = dict(ChainMap(*stats))

            # stats = {key: [val] for }

            print('DONE')
            return return_val

        except Exception:
            exc_type, exc_obj, exc_tb = sys.exc_info()
            return str(exc_obj.args[0]) + " FOUND IN LINE: " \
                + str(exc_tb.tb_lineno)


@contextlib.contextmanager
def tqdm_joblib(tqdm_object):
    """Context manager to patch joblib to report into tqdm 
    progress bar given as argument"""
    os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'

    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):
        def __call__(self, *args, **kwargs):

            tqdm_object.update(n=self.batch_size)
            return super().__call__(*args, **kwargs)

    old_batch_callback = joblib.parallel.BatchCompletionCallBack
    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback
    try:
        yield tqdm_object
    finally:
        joblib.parallel.BatchCompletionCallBack = old_batch_callback
        tqdm_object.close()


def read_pickle_parallel(pickle_path):

    df = pd.read_pickle(pickle_path)

    return df


if __name__ == "__main__":

    import warnings
    import os
    import sys
    import more_itertools as mit
    from openpyxl import load_workbook, Workbook
    import pandas as pd
    from scipy.io import loadmat as load_mat_scipy
    from joblib import Parallel, delayed, parallel_backend, parallel_config
    from tqdm import tqdm
    import contextlib
    import joblib
    from ray.util.joblib import register_ray

    warnings.filterwarnings("ignore")

    class_obj = DriverComfort()

    is_parallel_run = False  # make it true for HPCC / remote windows
    is_remote_windows = False  # make it true for remote windows desktop

    hpcc_cmd_run = True
    is_pickle = False
    dir_name = datetime.now().strftime('%d_%b_%Y__%H%M%S')

    program_name = 'thunder'  # 'cadm'  # 'gen6'
    log_type = 'new'

    if "win" in sys.platform:

        if is_remote_windows:

            req_dir = os.path.join(os.getcwd(), 'data', 'Thunder')
            # os.path.join(r'E:\Gen6_data\Dec_2023')

            back_end = "ray"  # 'loky'  #
            register_ray()

            file_names = [os.path.join(req_dir, file)
                          for file in os.listdir(req_dir)
                          if file.endswith(".mat")]
        else:

            if not is_parallel_run:

                log_name = \
                    'TNDR1_DUCK_20240412_112956_WDC4_dma_0010.mat'
                # '20231103_c815d121_DA_DEM_HMI_Route3_RBL_Run3_210326_025.mat'

                file_name = os.path.join(os.getcwd(), 'data',
                                         log_name)

                # mat_file = class_obj.loadmat(file_name)

                # out_df = class_obj.get_signals_gen6(mat_file)

                # (out_df, _,
                #  event_start_end_groups_2,
                #  host_lanechange) = class_obj.find_cut_in_indices_gen6(mat_file, 'new')

                # out_df_2, event_start_end_groups_2, \
                #     start_end_and_detection_indices_arr, \
                #     event_start_end_cTimes = class_obj._check_event_complete(
                #         mat_file, log_type='new')

                kwargs = {'local': True,
                          'program': program_name,
                          'log_type': log_type,
                          'path_to_save_pickle':
                              os.path.join(os.getcwd(), 'data',)
                          }

                excel_path = os.path.join(os.getcwd(), 'data',
                                          'sample.xlsx')

            #     program_name = kwargs['program']
            # log_type = kwargs['log_type']

                kwargs['config_path'] = os.path.join(os.getcwd(), 'data',
                                                     'config_thunder_driver_comfort.yaml')

                final_out = class_obj.run(file_name, **kwargs)

                final_out.to_excel(excel_path,
                                   sheet_name='output', index=False)

                fit_object = class_obj.kpi_sheet_generation(excel_path)
            else:

                req_dir = os.path.join(os.getcwd(), 'parallel_data', 'Thunder')
                file_names = [os.path.join(req_dir, file)
                              for file in os.listdir(req_dir)
                              if file.endswith(".mat")]

    elif "lin" in sys.platform:

        back_end = 'loky'  # "ray"

        is_pickle = True
        try:
            file_list_path = sys.argv[1]
        except:

            file_list_path = r'/mnt/usmidet/projects/FCA-CADM/2-Sim/USER_DATA/ioiny8' \
                + r'/Cut_in/Gen6/itc_gofast_sfw_all_mat_GLX_345_Dec5_2023'

        file_names = list(pd.read_csv(file_list_path, header=None)[0].values)

    if is_parallel_run:

        bool_val = True if hpcc_cmd_run else False

        kwargs_list = [{'local': bool_val,
                        'program': program_name,
                        'log_type': log_type,
                        'hpcc_cmd_run':
                            hpcc_cmd_run,
                        'output': dir_name,
                        'config_path':
                            os.path.join(os.getcwd(), 'data',
                                         'config_thunder_driver_comfort.yaml')
                        }
                       for _ in file_names]

        args = list(zip(file_names, kwargs_list))

        slice_length = 800
        args_sliced = list(mit.chunked(args, slice_length))
        num_jobs = -1

        # -1 :  all cores used intelligently
        # 1  : single core, i.e., no parallel
        # -n : all but n cores are used for parallel
        # n  : n cores used parallely

        def secondsToStr(t):
            return "%d:%02d:%02d.%03d" % \
                reduce(lambda ll, b: divmod(ll[0], b) + ll[1:],
                       [(t*1000,), 1000, 60, 60])

        start_time = time.time()
        if is_pickle:
            cwd = os.getcwd()
            req_path = os.path.join(cwd, dir_name)

            if not os.path.isdir(req_path):

                # if the demo_folder2 directory is
                # not present then create it.
                os.makedirs(req_path)

        results = []

        with parallel_config(backend=back_end):

            with tqdm_joblib(tqdm(desc="My calculation",
                                  total=len(args)
                                  )) as progress_bar:

                # os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'
                for i, args_ in enumerate(args_sliced):
                    results_ = Parallel(n_jobs=num_jobs,
                                        prefer='processes',
                                        # return_as="generator",
                                        )(delayed(
                                            class_obj.parallel_wrapper_run)(a)
                                          for a in args_)
                    if is_pickle:
                        results_ = [result for result in results_
                                    if isinstance(result, pd.DataFrame)]

                        if len(results_) == 0:
                            results_iter_df = pd.DataFrame(
                                columns=class_obj._headers['output_data'])
                        else:
                            results_iter_df = pd.concat(
                                results_, ignore_index=True)
                        req_file_path = os.path.join(req_path, str(i)+'.pkl')
                        results_iter_df.to_pickle(req_file_path,
                                                  protocol=5)
                    else:
                        results = results_

            # results = results_

        end_time = time.time()

        if is_pickle:
            path_list = file_names = [os.path.join(req_path, file)
                                      for file in os.listdir(req_path)
                                      if file.endswith(".pkl")]
            # read_pickle_parallel
            results = Parallel(n_jobs=num_jobs,
                               prefer='processes',
                               # return_as="generator",
                               )(delayed(
                                   read_pickle_parallel)(a)
                                 for a in path_list)

        results_refined = [result[0] for result in results
                           if isinstance(result[0], pd.DataFrame)]
        results_df = pd.concat(results_refined, ignore_index=True)

        results_df.to_csv(
            f"results_gen6_{dir_name}.csv")

        elapsed_time = secondsToStr(end_time-start_time)

        print(f'&&&&&&&&&&&& Elapsed time is {elapsed_time} %%%%%%%%%%%%%%%%')
